{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cell\n",
    "\n",
    "import xarray as xr # to work with multi-dimensional arrays\n",
    "import numpy as np # to work with multi-dimensional arrays\n",
    "import regionmask # to work with predefined data masks\n",
    "import pandas as pd # to work with tables\n",
    "import xesmf as xe # to regrid data\n",
    "import glob # to find file pathways\n",
    "import netCDF4 # use to save xarrays as nc files\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import cartopy.crs as ccrs # use for geographic map projections\n",
    "import time # to add a creation timestamp to Datasets\n",
    "import pickle # to save DataFrame objects efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Mean and the Standard Deviation of Absolute Annual Near Surface Temperature: All Models\n",
    "\n",
    "Raw monthly near surface temperature data has been accessed from the CEDA archives via JASMIN resources, and will be used to determine spatial mean annual near surface temperature and its associated standard deviation for a range of CMIP6 models.\n",
    "\n",
    "Metacode:\n",
    "\n",
    "1. Restrict Dataset/DataArray object to specified time period. (e.g. 2071-2100.)\n",
    "2. Compute the annual near surface temperature for each year within the period by averging the monthly values.\n",
    "3. Compute the mean annual near surface temperature and the standard deviation of these means across the period.\n",
    "4. Form a Dataset object with these two variables.\n",
    "5. Save Datatset object.\n",
    "\n",
    "As essentially only working out a time average across cells, no need for weighting to account for change in cell area with latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_tas(arr, start_year, end_year):\n",
    "    \n",
    "    '''\n",
    "    Summary:\n",
    "    --------\n",
    "    Computes the spatial mean and standard deviation of annual surface temperature from monthly data\n",
    "    across a specified range.\n",
    "    \n",
    "    Paramters:\n",
    "    ----------\n",
    "    arr: xarray dataset\n",
    "         xarray object must be contain monthly interval with surface temperature variable (tas)\n",
    "         xarray object must be chronologically ordered\n",
    "         \n",
    "    start_year: str\n",
    "                starting year of the temporal range to be used\n",
    "    \n",
    "    end_year: str\n",
    "              end year of the temporal range to be used\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ann_tas: xarray dataset\n",
    "                  \n",
    "    '''\n",
    "    # load air surface temperature variable in\n",
    "    tas_arr = arr.tas\n",
    "\n",
    "    # select only values between specified start and end years\n",
    "    tas_arr = tas_arr.sel(time = slice(start_year, end_year, 1)) # use slice to index the time dimension\n",
    "\n",
    "    # convert temperature units from Kelvin to degrees Celcius\n",
    "    tas_arr = tas_arr - 273.15\n",
    "\n",
    "    # compute the yearly spatial mean temperature for each year; average monthly temperatures for given year\n",
    "    tas_arr = tas_arr.groupby('time.year').mean(skipna = True)\n",
    "\n",
    "    # compute the spatial mean annual temperature across the specified period\n",
    "    mean_tas = tas_arr.mean(dim = 'year', skipna = True)\n",
    "\n",
    "    # compute the spatial standard deviation of the mean annual temperature across the specified period\n",
    "    std_dev_tas = tas_arr.std(dim = 'year', skipna = True)\n",
    "\n",
    "    # rename the DataArray objects\n",
    "    mean_tas = mean_tas.rename('mean_ann_tas')\n",
    "    std_dev_tas = std_dev_tas.rename('std_dev_ann_tas')\n",
    "\n",
    "    # define some attributes for the mean and standard deviation DataArray objects\n",
    "    mean_tas.attrs['units'], std_dev_tas.attrs['units'] = 'degrees Celcius', 'degrees Celcius'\n",
    "    mean_tas.attrs['sample_start_year'], std_dev_tas.attrs['sample_start_year'] = start_year, start_year\n",
    "    mean_tas.attrs['sample_end_year'], std_dev_tas.attrs['sample_end_year'] = end_year, end_year\n",
    "    mean_tas.attrs['model'], std_dev_tas.attrs['model'] = arr.source_id, arr.source_id\n",
    "    mean_tas.attrs['modelling_group'], std_dev_tas.attrs['modelling_group'] = arr.institution, arr.institution\n",
    "    mean_tas.attrs['modelling_group_id'], std_dev_tas.attrs['modelling_group_id'] = arr.institution_id, \\\n",
    "                                                                                    arr.institution_id\n",
    "    mean_tas.attrs['experiment'], std_dev_tas.attrs['experiment'] = arr.experiment_id, arr.experiment_id\n",
    "    mean_tas.attrs['realisation'], std_dev_tas.attrs['realisation'] = arr.variant_label, arr.variant_label\n",
    "\n",
    "    # create Dataset object from the mean and standard deviation DataArray objects\n",
    "    ann_tas = xr.Dataset({'mean_tas': mean_tas, 'std_dev_tas': std_dev_tas})\n",
    "\n",
    "    # define some attributes for the Dataset object\n",
    "    ann_tas.attrs['comment'] = 'spatial annual mean and standard deviation of near surface temperature'\n",
    "    ann_tas.attrs['units'] = 'degrees Celcius'\n",
    "    ann_tas.attrs['sample_start_year'] = start_year\n",
    "    ann_tas.attrs['sample_end_year'] = end_year\n",
    "    ann_tas.attrs['model'] = arr.source_id\n",
    "    ann_tas.attrs['modelling_group'] = arr.institution\n",
    "    ann_tas.attrs['modelling_group_id'] = arr.institution_id\n",
    "    ann_tas.attrs['experiment'] = arr.experiment_id\n",
    "    ann_tas.attrs['realisation'] = arr.variant_label\n",
    "    ann_tas.attrs['created'] = time.ctime()\n",
    "    \n",
    "    # close open Dataset and/or DataArray objects\n",
    "    mean_tas.close(), std_dev_tas.close(), tas_arr.close(), arr.close()\n",
    "    \n",
    "    return ann_tas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_arrays_via_time(pw_list):\n",
    "    \n",
    "    '''\n",
    "    Summary:\n",
    "    --------\n",
    "    Merges xarrays with the same attributes along the time dimension.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pw_list: list\n",
    "             list containing the absolute pathways of the xarrays to be merged\n",
    "             \n",
    "    Returns:\n",
    "    --------\n",
    "    merged_array: xarray\n",
    "                  chronologically ordered xarray containing all arrays listed in pw_list\n",
    "    \n",
    "    '''\n",
    "    # load first array\n",
    "    x = xr.open_dataset(pw_list[0], use_cftime = True) # cftime is a package that decodes netCDF files times\n",
    "                                                              # times to conform with climate and forecasting times\n",
    "    merged_array = x\n",
    "    x.close()\n",
    "    del(x)\n",
    "\n",
    "    # merge remaining arrays\n",
    "    for pw in pw_list[1: ]:\n",
    "        x = xr.open_dataset(pw, use_cftime = True).load()\n",
    "        merged_array = xr.concat([merged_array, x], dim = 'time')\n",
    "        x.close()\n",
    "        del(x)\n",
    "        \n",
    "    # sort resultant array by time; earliest to latest\n",
    "    merged_array = merged_array.sortby('time')\n",
    "\n",
    "    return merged_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2 done.\n",
      "ACCESS-ESM1-5 done.\n",
      "AWI-CM-1-1-MR done.\n",
      "BCC-CSM2-MR done.\n",
      "CanESM5 done.\n",
      "CNRM-CM6-1 done.\n",
      "GFDL-ESM4 done.\n",
      "INM-CM4-8 done.\n",
      "INM-CM5-0 done.\n",
      "IPSL-CM6A-LR done.\n",
      "MIROC6 done.\n",
      "MPI-ESM1-2-HR done.\n",
      "MRI-ESM2-0 done.\n",
      "NorESM2-MM done.\n",
      "UKESM1-0-LL done.\n"
     ]
    }
   ],
   "source": [
    "# define a list containing the CMIP6 models used in study\n",
    "models = ['ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CanESM5',\n",
    "          'CNRM-CM6-1', 'GFDL-ESM4', 'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'MIROC6',\n",
    "          'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM', 'UKESM1-0-LL']\n",
    "\n",
    "# calculate the spatial mean and standard deviation for all periods for each model\n",
    "for model in models:\n",
    "\n",
    "    # define the pathways to original CMIP6 near surface temperature data\n",
    "    pw_585 = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/ssp585/*{model}*')\n",
    "    pw_370 = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/ssp370/*{model}*')\n",
    "    pw_245 = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/ssp245/*{model}*')\n",
    "    pw_126 = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/ssp126/*{model}*')\n",
    "    pw_hist = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/historic/*{model}*')\n",
    "   \n",
    "    # compute the spatial mean and standard deviation of the annual near surface temperature\n",
    "    ds_585 = ann_tas(merge_arrays_via_time(pw_585), '2071', '2100')\n",
    "    ds_370 = ann_tas(merge_arrays_via_time(pw_370), '2071', '2100')\n",
    "    ds_245 = ann_tas(merge_arrays_via_time(pw_245), '2071', '2100')\n",
    "    ds_126 = ann_tas(merge_arrays_via_time(pw_126), '2071', '2100')\n",
    "    ds_preind = ann_tas(merge_arrays_via_time(pw_hist), '1851', '1900')\n",
    "    ds_cur = ann_tas(merge_arrays_via_time(pw_hist), '1981', '2010')\n",
    "    \n",
    "    # define pathways to save Dataset objects\n",
    "    if model == 'MPI-ESM1-2-HR': # slight workaround due to variation of MPI modelling group names\n",
    "        spw_585 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_585.experiment}' + \\\n",
    "                  f'/MPI-M/{ds_585.model}' + \\\n",
    "                  f'/{ds_585.experiment}_{ds_585.model}_ann_tas_{ds_585.sample_start_year}-{ds_585.sample_end_year}.nc'\n",
    "        spw_370 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_370.experiment}' + \\\n",
    "                  f'/MPI-M/{ds_370.model}' + \\\n",
    "                  f'/{ds_370.experiment}_{ds_370.model}_ann_tas_{ds_370.sample_start_year}-{ds_370.sample_end_year}.nc'\n",
    "        spw_245 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_245.experiment}' + \\\n",
    "                  f'/MPI-M/{ds_245.model}' + \\\n",
    "                  f'/{ds_245.experiment}_{ds_245.model}_ann_tas_{ds_245.sample_start_year}-{ds_245.sample_end_year}.nc'\n",
    "        spw_126 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_126.experiment}' + \\\n",
    "                  f'/MPI-M/{ds_126.model}' + \\\n",
    "                  f'/{ds_126.experiment}_{ds_126.model}_ann_tas_{ds_126.sample_start_year}-{ds_126.sample_end_year}.nc'\n",
    "        spw_cur = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/current' + \\\n",
    "                  f'/MPI-M/{ds_cur.model}' + \\\n",
    "                  f'/current_{ds_cur.model}_ann_tas_{ds_cur.sample_start_year}-{ds_cur.sample_end_year}.nc'\n",
    "        spw_preind = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/pre_ind' + \\\n",
    "                     f'/MPI-M/{ds_preind.model}' + \\\n",
    "                     f'/pre_ind_{ds_preind.model}_ann_tas_{ds_preind.sample_start_year}' + \\\n",
    "                     f'-{ds_preind.sample_end_year}.nc'\n",
    "        \n",
    "    else:\n",
    "        spw_585 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_585.experiment}' + \\\n",
    "                  f'/{ds_585.modelling_group_id}/{ds_585.model}' + \\\n",
    "                  f'/{ds_585.experiment}_{ds_585.model}_ann_tas_{ds_585.sample_start_year}-{ds_585.sample_end_year}.nc'\n",
    "        spw_370 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_370.experiment}' + \\\n",
    "                  f'/{ds_370.modelling_group_id}/{ds_370.model}' + \\\n",
    "                  f'/{ds_370.experiment}_{ds_370.model}_ann_tas_{ds_370.sample_start_year}-{ds_370.sample_end_year}.nc'\n",
    "        spw_245 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_245.experiment}' + \\\n",
    "                  f'/{ds_245.modelling_group_id}/{ds_245.model}' + \\\n",
    "                  f'/{ds_245.experiment}_{ds_245.model}_ann_tas_{ds_245.sample_start_year}-{ds_245.sample_end_year}.nc'\n",
    "        spw_126 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_126.experiment}' + \\\n",
    "                  f'/{ds_126.modelling_group_id}/{ds_126.model}' + \\\n",
    "                  f'/{ds_126.experiment}_{ds_126.model}_ann_tas_{ds_126.sample_start_year}-{ds_126.sample_end_year}.nc'\n",
    "        spw_cur = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/current' + \\\n",
    "                  f'/{ds_cur.modelling_group_id}/{ds_cur.model}' + \\\n",
    "                  f'/current_{ds_cur.model}_ann_tas_{ds_cur.sample_start_year}-{ds_cur.sample_end_year}.nc'\n",
    "        spw_preind = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/pre_ind' + \\\n",
    "                     f'/{ds_preind.modelling_group_id}/{ds_preind.model}' + \\\n",
    "                     f'/pre_ind_{ds_preind.model}_ann_tas_{ds_preind.sample_start_year}' + \\\n",
    "                     f'-{ds_preind.sample_end_year}.nc'\n",
    "    \n",
    "    # save Dataset objects to defined pathways\n",
    "    ds_585.to_netcdf(spw_585, mode = 'w')\n",
    "    ds_370.to_netcdf(spw_370, mode = 'w')\n",
    "    ds_245.to_netcdf(spw_245, mode = 'w')\n",
    "    ds_126.to_netcdf(spw_126, mode = 'w')\n",
    "    ds_cur.to_netcdf(spw_cur, mode = 'w')\n",
    "    ds_preind.to_netcdf(spw_preind, mode = 'w')\n",
    "\n",
    "    # close Dataset objects\n",
    "    ds_585.close(), ds_370.close(), ds_245.close(),\n",
    "    ds_126.close(), ds_cur.close(), ds_preind.close()\n",
    "    \n",
    "    print(f'{model} done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Mean and the Standard Deviation of  Annual Near Surface Temperature Anomaly: All Models\n",
    "\n",
    "Raw monthly near surface temperature data has been accessed from the CEDA archives via JASMIN resources, and will be used to determine spatial mean annual near surface temperature anomaly and its associated standard deviation for a range of CMIP6 models.\n",
    "\n",
    "Metacode:\n",
    "\n",
    "1. Define baseline period to obtain the pre-industrial climatology from.\n",
    "2. Compute the mean monthly temperatures across baseline period.\n",
    "3. Compute the temperature anomalies in the projections by negating the baseline monthly values.\n",
    "4. Compute the mean annual temperature anomaly and the standard deviation.\n",
    "5. Form a Dataset object with these two variables.\n",
    "6. Save Dataset object. (Ensure anomaly and 1851-1900 are in file name.)\n",
    "\n",
    "As essentially only working out a time average across cells, no need for weighting to account for change in cell area with latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_anom_tas(bl_arr, pj_arr, start_year, end_year):\n",
    "    \n",
    "    '''\n",
    "    Summary:\n",
    "    --------\n",
    "    Computes the spatial mean and standard deviation of the annual surface temperature anomaly \n",
    "    from monthly data across a specified range. Anomaly is relative to a pre-industrial baseline\n",
    "    (1851-1900)\n",
    "    \n",
    "    Paramters:\n",
    "    ----------\n",
    "    bl_arr: xarray Dataset object\n",
    "            xarray object must be contain monthly interval with surface temperature variable (tas)\n",
    "            xarray object must be chronologically ordered\n",
    "            xarray object must contain at least data between 1851-1900\n",
    "            \n",
    "    pj_arr: xarray Dataset object\n",
    "            xarray object must be contain monthly interval with surface temperature variable (tas)\n",
    "            xarray object must be chronologically ordered\n",
    "               \n",
    "    start_year: str\n",
    "                starting year of the temporal range to be used\n",
    "    \n",
    "    end_year: str\n",
    "              end year of the temporal range to be used\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    anom_tas: xarray Dataset object\n",
    "                  \n",
    "    '''\n",
    "    # load air surface temperature variable from both projection and baseline arrays\n",
    "    pj_tas_arr, bl_tas_arr = pj_arr.tas, bl_arr.tas\n",
    "\n",
    "    # convert temperautre from Kevlin to degree Celcius\n",
    "    pj_tas_arr, bl_tas_arr = pj_tas_arr - 273.15, bl_tas_arr - 273.15\n",
    "\n",
    "    # deterime baseline monthly temperature values; baseline period is 1851-1900\n",
    "    bl_tas_arr = bl_tas_arr.sel(time = slice('1851', '1900', 1)) # use slice to index the time dimension\n",
    "    monthly_bl = bl_tas_arr.groupby('time.month').mean('time', skipna = True)\n",
    "\n",
    "    # restrict projection array to specified years\n",
    "    pj_tas_arr = pj_tas_arr.sel(time = slice(start_year, end_year, 1))\n",
    "\n",
    "    # compute monthly anomalies across entire period\n",
    "    anomalies = pj_tas_arr.groupby('time.month') - monthly_bl\n",
    "\n",
    "    # compute the yearly spatial mean anomaly for each year\n",
    "    anomalies = anomalies.groupby('time.year').mean()\n",
    "\n",
    "    # compute the spatial mean annual anomaly\n",
    "    ann_tas_anom = anomalies.mean(dim = 'year', skipna = True)\n",
    "\n",
    "    # compute the standard deviation of the spatial mean annual anomaly; the intra-annual variability\n",
    "    std_dev_ann_tas_anom = anomalies.std(dim = 'year', skipna = True)\n",
    "\n",
    "    # define some attributes for the mean and standard deviation DataArray objects\n",
    "    ann_tas_anom.attrs['units'], ann_tas_anom.attrs['sample_start_year'] = 'degrees Celcius', start_year\n",
    "    ann_tas_anom.attrs['sample_end_year'], ann_tas_anom.attrs['model'] = end_year, pj_arr.source_id\n",
    "    ann_tas_anom.attrs['modelling_group'] = pj_arr.institution\n",
    "    ann_tas_anom.attrs['modelling_group_id'] = pj_arr.institution_id\n",
    "    ann_tas_anom.attrs['experiment'] = pj_arr.experiment_id\n",
    "    ann_tas_anom.attrs['realisation'] =  pj_arr.variant_label\n",
    "    std_dev_ann_tas_anom.attrs['units'], std_dev_ann_tas_anom.attrs['sample_start_year'] = 'degrees Celcius', start_year\n",
    "    std_dev_ann_tas_anom.attrs['sample_end_year'], std_dev_ann_tas_anom.attrs['model'] = end_year, pj_arr.source_id\n",
    "    std_dev_ann_tas_anom.attrs['modelling_group'] = pj_arr.institution\n",
    "    std_dev_ann_tas_anom.attrs['modelling_group_id'] = pj_arr.institution_id\n",
    "    std_dev_ann_tas_anom.attrs['experiment'] = pj_arr.experiment_id\n",
    "    std_dev_ann_tas_anom.attrs['realisation'] =  pj_arr.variant_label\n",
    "\n",
    "    # create Dataset object to hold anomaly mean and standard deviation DataArray objects\n",
    "    anom_tas = xr.Dataset({'ann_tas_anom': ann_tas_anom,\n",
    "                           'std_dev_ann_tas_anom': std_dev_ann_tas_anom})\n",
    "\n",
    "    # define some attributes for the Dataset object\n",
    "    anom_tas.attrs['comment'] = 'spatial annual anomaly mean and standard deviation of near surface temperature.'\n",
    "    anom_tas.attrs['units'] = 'degrees Celcius'\n",
    "    anom_tas.attrs['sample_start_year'], anom_tas.attrs['sample_end_year'] = start_year, end_year\n",
    "    anom_tas.attrs['model'], anom_tas.attrs['modelling_group'] = pj_arr.source_id, pj_arr.institution\n",
    "    anom_tas.attrs['modelling_group_id'], anom_tas.attrs['experiment'] = pj_arr.institution_id, pj_arr.experiment_id\n",
    "    anom_tas.attrs['realisation'], anom_tas.attrs['created'] = pj_arr.variant_label, time.ctime()\n",
    "\n",
    "    # close DataArray objects\n",
    "    pj_tas_arr.close(), bl_tas_arr.close(), pj_arr.close(), bl_arr.close()\n",
    "    monthly_bl.close(), anomalies.close(), ann_tas_anom.close(), std_dev_ann_tas_anom.close()\n",
    "    \n",
    "    return anom_tas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2 done.\n",
      "ACCESS-ESM1-5 done.\n",
      "AWI-CM-1-1-MR done.\n",
      "BCC-CSM2-MR done.\n",
      "CanESM5 done.\n",
      "CNRM-CM6-1 done.\n",
      "GFDL-ESM4 done.\n",
      "INM-CM4-8 done.\n",
      "INM-CM5-0 done.\n",
      "IPSL-CM6A-LR done.\n",
      "MIROC6 done.\n",
      "MPI-ESM1-2-HR done.\n",
      "MRI-ESM2-0 done.\n",
      "NorESM2-MM done.\n",
      "UKESM1-0-LL done.\n"
     ]
    }
   ],
   "source": [
    "# define a list containing the CMIP6 models used in study\n",
    "models = ['ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CanESM5',\n",
    "          'CNRM-CM6-1', 'GFDL-ESM4', 'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'MIROC6',\n",
    "          'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM', 'UKESM1-0-LL']\n",
    "\n",
    "# calculate the spatial mean anomaly and standard deviation for all periods for each model\n",
    "for model in models:\n",
    "\n",
    "    # define the pathways to original CMIP6 near surface temperature data\n",
    "    pw_585 = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/ssp585/*{model}*')\n",
    "    pw_370 = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/ssp370/*{model}*')\n",
    "    pw_245 = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/ssp245/*{model}*')\n",
    "    pw_126 = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/ssp126/*{model}*')\n",
    "    pw_hist = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/historic/*{model}*')\n",
    "   \n",
    "    # compute the spatial mean anomaly and standard deviation of the annual near surface temperature\n",
    "    ds_585 = ann_anom_tas(merge_arrays_via_time(pw_hist), merge_arrays_via_time(pw_585), '2071', '2100')\n",
    "    ds_370 = ann_anom_tas(merge_arrays_via_time(pw_hist), merge_arrays_via_time(pw_370), '2071', '2100')\n",
    "    ds_245 = ann_anom_tas(merge_arrays_via_time(pw_hist), merge_arrays_via_time(pw_245), '2071', '2100')\n",
    "    ds_126 = ann_anom_tas(merge_arrays_via_time(pw_hist), merge_arrays_via_time(pw_126), '2071', '2100')\n",
    "    ds_cur = ann_anom_tas(merge_arrays_via_time(pw_hist), merge_arrays_via_time(pw_hist), '1981', '2010')\n",
    "    \n",
    "    # define pathways to save Dataset objects\n",
    "    if model == 'MPI-ESM1-2-HR': # slight workaround due to variation of MPI modelling group names\n",
    "        spw_585 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_585.experiment}' + \\\n",
    "                  f'/MPI-M/{ds_585.model}' + \\\n",
    "                  f'/{ds_585.experiment}_{ds_585.model}_anomaly_tas_{ds_585.sample_start_year}-{ds_585.sample_end_year}' + \\\n",
    "                  '_relative_1851-1900.nc'\n",
    "        spw_370 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_370.experiment}' + \\\n",
    "                  f'/MPI-M/{ds_370.model}' + \\\n",
    "                  f'/{ds_370.experiment}_{ds_370.model}_anomaly_tas_{ds_370.sample_start_year}-{ds_370.sample_end_year}' + \\\n",
    "                  '_relative_1851-1900.nc'\n",
    "        spw_245 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_245.experiment}' + \\\n",
    "                  f'/MPI-M/{ds_245.model}' + \\\n",
    "                  f'/{ds_245.experiment}_{ds_245.model}_anomaly_tas_{ds_245.sample_start_year}-{ds_245.sample_end_year}' + \\\n",
    "                  '_relative_1851-1900.nc'\n",
    "        spw_126 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_126.experiment}' + \\\n",
    "                  f'/MPI-M/{ds_126.model}' + \\\n",
    "                  f'/{ds_126.experiment}_{ds_126.model}_anomaly_tas_{ds_126.sample_start_year}-{ds_126.sample_end_year}' + \\\n",
    "                  '_relative_1851-1900.nc'\n",
    "        spw_cur = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/current' + \\\n",
    "                  f'/MPI-M/{ds_cur.model}' + \\\n",
    "                  f'/current_{ds_cur.model}_anomaly_tas_{ds_cur.sample_start_year}-{ds_cur.sample_end_year}' + \\\n",
    "                  '_relative_1851-1900.nc'\n",
    "        \n",
    "    else:\n",
    "        spw_585 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_585.experiment}' + \\\n",
    "                  f'/{ds_585.modelling_group_id}/{ds_585.model}' + \\\n",
    "                  f'/{ds_585.experiment}_{ds_585.model}_anomaly_tas_{ds_585.sample_start_year}-{ds_585.sample_end_year}' + \\\n",
    "                  '_relative_1851-1900.nc'\n",
    "        spw_370 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_370.experiment}' + \\\n",
    "                  f'/{ds_370.modelling_group_id}/{ds_370.model}' + \\\n",
    "                  f'/{ds_370.experiment}_{ds_370.model}_anomaly_tas_{ds_370.sample_start_year}-{ds_370.sample_end_year}' + \\\n",
    "                  '_relative_1851-1900.nc'\n",
    "        spw_245 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_245.experiment}' + \\\n",
    "                  f'/{ds_245.modelling_group_id}/{ds_245.model}' + \\\n",
    "                  f'/{ds_245.experiment}_{ds_245.model}_anomaly_tas_{ds_245.sample_start_year}-{ds_245.sample_end_year}' + \\\n",
    "                  '_relative_1851-1900.nc'\n",
    "        spw_126 = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/{ds_126.experiment}' + \\\n",
    "                  f'/{ds_126.modelling_group_id}/{ds_126.model}' + \\\n",
    "                  f'/{ds_126.experiment}_{ds_126.model}_anomaly_tas_{ds_126.sample_start_year}-{ds_126.sample_end_year}' + \\\n",
    "                  '_relative_1851-1900.nc'\n",
    "        spw_cur = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/current' + \\\n",
    "                  f'/{ds_cur.modelling_group_id}/{ds_cur.model}' + \\\n",
    "                  f'/current_{ds_cur.model}_anomaly_tas_{ds_cur.sample_start_year}-{ds_cur.sample_end_year}' + \\\n",
    "                  '_relative_1851-1900.nc'\n",
    "    \n",
    "    # save Dataset objects to defined pathways\n",
    "    ds_585.to_netcdf(spw_585, mode = 'w')\n",
    "    ds_370.to_netcdf(spw_370, mode = 'w')\n",
    "    ds_245.to_netcdf(spw_245, mode = 'w')\n",
    "    ds_126.to_netcdf(spw_126, mode = 'w')\n",
    "    ds_cur.to_netcdf(spw_cur, mode = 'w')\n",
    "    ds_preind.to_netcdf(spw_preind, mode = 'w')\n",
    "\n",
    "    # close Dataset objects\n",
    "    ds_585.close(), ds_370.close(), ds_245.close(),\n",
    "    ds_126.close(), ds_cur.close(), ds_preind.close()\n",
    "    \n",
    "    print(f'{model} done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'tas' (time: 420, lat: 144, lon: 192)>\n",
      "[11612160 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * time     (time) object 2015-01-16 00:00:00 ... 2049-12-16 00:00:00\n",
      "  * lat      (lat) float64 -89.38 -88.12 -86.88 -85.62 ... 86.88 88.12 89.38\n",
      "  * lon      (lon) float64 0.9375 2.812 4.688 6.562 ... 353.4 355.3 357.2 359.1\n",
      "    height   float64 ...\n",
      "Attributes:\n",
      "    standard_name:  air_temperature\n",
      "    long_name:      Near-Surface Air Temperature\n",
      "    comment:        near-surface (usually, 2 meter) air temperature\n",
      "    units:          K\n",
      "    original_name:  mo: (stash: m01s03i236, lbproc: 128)\n",
      "    cell_methods:   area: time: mean\n",
      "    cell_measures:  area: areacella\n",
      "    history:        2019-04-18T14:32:46Z altered by CMOR: Treated scalar dime...\n"
     ]
    }
   ],
   "source": [
    "pw_585 = glob.glob(f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/working_files/ssp585/*UKESM*-2049*')\n",
    "x = xr.open_dataset(pw_585[0])\n",
    "print(x.tas)\n",
    "x.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regridding of Model Output: 1.0x1.0 and 5.0x5.0 degree latitude and longitude grids\n",
    "\n",
    "Metacode:\n",
    "1. Define desired resolution of data to be regridded to.\n",
    "2. Read in original resolution data.\n",
    "3. Compute a \"regridder\" which will transform the data from one resolution to another.\n",
    "4. Save regridded data to the directory containing the original resolution data.\n",
    "\n",
    "Data regridded to both 1x1 and 5.0x5.0 grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define absolute and anomaly near surface temperature data to regrid; both end in '0.nc'\n",
    "f_pws = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/*/*/*/*0.nc')\n",
    "\n",
    "# define desired output grid; 1.0 x 1.0 degrees\n",
    "grid_out = xr.Dataset({'lat': (['lat'], np.arange(-89.5, 90.5, 1.)),\n",
    "                       'lon': (['lon'], np.arange(-179.5, 180.5, 1.))})\n",
    "\n",
    "for pw in f_pws:\n",
    "    \n",
    "    # load in near surface temperature data in the original resolution\n",
    "    grid_in = xr.open_dataset(pw)\n",
    "    \n",
    "    # compute the \"regridder\" file which will define the weighting to apply\n",
    "    regridder = xe.Regridder(grid_in, grid_out,\n",
    "                             method = 'bilinear', # interpolation method\n",
    "                             periodic = True) # required for global girds; prevents blank data on meridian\n",
    "    \n",
    "    # apply the weighting matrix to transform the data to new resolution\n",
    "    rg_data = regridder(grid_in)\n",
    "    \n",
    "    # clear regridder file from being saved\n",
    "    regridder.clean_weight_file()\n",
    "    \n",
    "    # define some atrributes for the new resolution Dataset object\n",
    "    rg_data.attrs['units'] = grid_in.units\n",
    "    rg_data.attrs['period'] = f'{grid_in.sample_start_year}-{grid_in.sample_end_year}'\n",
    "    rg_data.attrs['model'], rg_data.attrs['modelling_group'] = grid_in.model, grid_in.modelling_group\n",
    "    rg_data.attrs['experiment'], rg_data.attrs['realisation'] = grid_in.experiment, grid_in.realisation\n",
    "    rg_data.attrs['resolution'], rg_data.attrs['created_on'] = 'lon-lat: 1x1 degrees', time.ctime()\n",
    "    \n",
    "    # save resulting dataset\n",
    "    save_pw = pw[: -3] + '_1x1_res.nc'\n",
    "    rg_data.to_netcdf(save_pw, mode = 'w')\n",
    "    print(f'File saved: {save_pw.rsplit(\"/\")[-1]}')\n",
    "\n",
    "    # close open Datasets\n",
    "    grid_out.close(), grid_in.close(), rg_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: ssp126_BCC-CSM2-MR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: ssp126_UKESM1-0-LL_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: ssp126_MRI-ESM2-0_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_64x128_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_64x128_36x72_peri.nc\n",
      "File saved: ssp126_CanESM5_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_143x144_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_143x144_36x72_peri.nc\n",
      "File saved: ssp126_IPSL-CM6A-LR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: ssp126_CNRM-CM6-1_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: ssp126_MIROC6_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: ssp126_AWI-CM-1-1-MR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_145x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_145x192_36x72_peri.nc\n",
      "File saved: ssp126_ACCESS-ESM1-5_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: ssp126_ACCESS-CM2_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: ssp126_INM-CM5-0_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: ssp126_INM-CM4-8_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x288_36x72_peri.nc\n",
      "File saved: ssp126_NorESM2-MM_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_180x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_180x288_36x72_peri.nc\n",
      "File saved: ssp126_GFDL-ESM4_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: ssp126_MPI-ESM1-2-HR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: ssp245_UKESM1-0-LL_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: ssp245_BCC-CSM2-MR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: ssp245_MRI-ESM2-0_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_64x128_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_64x128_36x72_peri.nc\n",
      "File saved: ssp245_CanESM5_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_143x144_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_143x144_36x72_peri.nc\n",
      "File saved: ssp245_IPSL-CM6A-LR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: ssp245_CNRM-CM6-1_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: ssp245_MIROC6_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: ssp245_AWI-CM-1-1-MR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_145x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_145x192_36x72_peri.nc\n",
      "File saved: ssp245_ACCESS-ESM1-5_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: ssp245_ACCESS-CM2_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: ssp245_INM-CM5-0_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: ssp245_INM-CM4-8_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x288_36x72_peri.nc\n",
      "File saved: ssp245_NorESM2-MM_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_180x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_180x288_36x72_peri.nc\n",
      "File saved: ssp245_GFDL-ESM4_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: ssp245_MPI-ESM1-2-HR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: ssp370_UKESM1-0-LL_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: ssp370_MRI-ESM2-0_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_64x128_36x72_peri.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_64x128_36x72_peri.nc\n",
      "File saved: ssp370_CanESM5_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_143x144_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_143x144_36x72_peri.nc\n",
      "File saved: ssp370_IPSL-CM6A-LR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: ssp370_CNRM-CM6-1_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: ssp370_BCC-CSM2-MR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: ssp370_MIROC6_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: ssp370_AWI-CM-1-1-MR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_145x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_145x192_36x72_peri.nc\n",
      "File saved: ssp370_ACCESS-ESM1-5_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: ssp370_ACCESS-CM2_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: ssp370_INM-CM5-0_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: ssp370_INM-CM4-8_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x288_36x72_peri.nc\n",
      "File saved: ssp370_NorESM2-MM_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_180x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_180x288_36x72_peri.nc\n",
      "File saved: ssp370_GFDL-ESM4_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: ssp370_MPI-ESM1-2-HR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: ssp585_UKESM1-0-LL_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: ssp585_BCC-CSM2-MR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: ssp585_MRI-ESM2-0_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_64x128_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_64x128_36x72_peri.nc\n",
      "File saved: ssp585_CanESM5_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_143x144_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_143x144_36x72_peri.nc\n",
      "File saved: ssp585_IPSL-CM6A-LR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: ssp585_CNRM-CM6-1_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: ssp585_MIROC6_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: ssp585_AWI-CM-1-1-MR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_145x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_145x192_36x72_peri.nc\n",
      "File saved: ssp585_ACCESS-ESM1-5_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: ssp585_ACCESS-CM2_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: ssp585_INM-CM5-0_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: ssp585_INM-CM4-8_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x288_36x72_peri.nc\n",
      "File saved: ssp585_NorESM2-MM_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_180x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_180x288_36x72_peri.nc\n",
      "File saved: ssp585_GFDL-ESM4_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: ssp585_MPI-ESM1-2-HR_ann_tas_2071-2100_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: pre_ind_AWI-CM-1-1-MR_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_64x128_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_64x128_36x72_peri.nc\n",
      "File saved: pre_ind_CanESM5_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_145x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_145x192_36x72_peri.nc\n",
      "File saved: pre_ind_ACCESS-ESM1-5_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: pre_ind_MPI-ESM1-2-HR_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: pre_ind_INM-CM5-0_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: pre_ind_INM-CM4-8_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: pre_ind_MIROC6_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: pre_ind_MRI-ESM2-0_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_180x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_180x288_36x72_peri.nc\n",
      "File saved: pre_ind_GFDL-ESM4_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: pre_ind_BCC-CSM2-MR_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: pre_ind_CNRM-CM6-1_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: pre_ind_ACCESS-CM2_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_143x144_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_143x144_36x72_peri.nc\n",
      "File saved: pre_ind_IPSL-CM6A-LR_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: pre_ind_UKESM1-0-LL_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_192x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x288_36x72_peri.nc\n",
      "File saved: pre_ind_NorESM2-MM_ann_tas_1851-1900_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: current_AWI-CM-1-1-MR_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_64x128_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_64x128_36x72_peri.nc\n",
      "File saved: current_CanESM5_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_145x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_145x192_36x72_peri.nc\n",
      "File saved: current_ACCESS-ESM1-5_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_192x384_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x384_36x72_peri.nc\n",
      "File saved: current_MPI-ESM1-2-HR_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: current_INM-CM5-0_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_120x180_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_120x180_36x72_peri.nc\n",
      "File saved: current_INM-CM4-8_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: current_MIROC6_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: current_MRI-ESM2-0_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_180x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_180x288_36x72_peri.nc\n",
      "File saved: current_GFDL-ESM4_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_160x320_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_160x320_36x72_peri.nc\n",
      "File saved: current_BCC-CSM2-MR_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_128x256_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_128x256_36x72_peri.nc\n",
      "File saved: current_CNRM-CM6-1_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: current_ACCESS-CM2_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_143x144_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_143x144_36x72_peri.nc\n",
      "File saved: current_IPSL-CM6A-LR_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_144x192_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_144x192_36x72_peri.nc\n",
      "File saved: current_UKESM1-0-LL_ann_tas_1981-2010_5x5_res.nc\n",
      "Create weight file: bilinear_192x288_36x72_peri.nc\n",
      "using dimensions ('lat', 'lon') from data variable mean_tas as the horizontal dimensions for this dataset.\n",
      "Remove file bilinear_192x288_36x72_peri.nc\n",
      "File saved: current_NorESM2-MM_ann_tas_1981-2010_5x5_res.nc\n"
     ]
    }
   ],
   "source": [
    "# define absolute near surface temperature data to regrid; both end in '0.nc'\n",
    "f_pws = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/*/*/*/*_ann_tas_*0.nc')\n",
    "\n",
    "# define desired output grid; 5.0 x 5.0 latitude-longitude grid\n",
    "grid_out = xr.Dataset({'lat': (['lat'], np.arange(-87.5, 92.5, 5.)),\n",
    "                       'lon': (['lon'], np.arange(-177.5, 182.5, 5.))})\n",
    "\n",
    "for pw in f_pws:\n",
    "    \n",
    "    # load in near surface temperature data in the original resolution\n",
    "    grid_in = xr.open_dataset(pw)\n",
    "    \n",
    "    # compute the \"regridder\" file which will define the weighting to apply\n",
    "    regridder = xe.Regridder(grid_in, grid_out,\n",
    "                             method = 'bilinear', # interpolation method\n",
    "                             periodic = True) # required for global girds; prevents blank data on meridian\n",
    "    \n",
    "    # apply the weighting matrix to transform the data to new resolution\n",
    "    rg_data = regridder(grid_in)\n",
    "    \n",
    "    # clear regridder file from being saved\n",
    "    regridder.clean_weight_file()\n",
    "    \n",
    "    # define some atrributes for the new resolution Dataset object\n",
    "    rg_data.attrs['units'] = grid_in.units\n",
    "    rg_data.attrs['period'] = f'{grid_in.sample_start_year}-{grid_in.sample_end_year}'\n",
    "    rg_data.attrs['model'], rg_data.attrs['modelling_group'] = grid_in.model, grid_in.modelling_group\n",
    "    rg_data.attrs['experiment'], rg_data.attrs['realisation'] = grid_in.experiment, grid_in.realisation\n",
    "    rg_data.attrs['resolution'], rg_data.attrs['created_on'] = 'lat-lon: 5x5 degrees', time.ctime()\n",
    "    \n",
    "    # save resulting dataset\n",
    "    save_pw = pw[: -3] + '_5x5_res.nc'\n",
    "    rg_data.to_netcdf(save_pw, mode = 'w')\n",
    "    print(f'File saved: {save_pw.rsplit(\"/\")[-1]}')\n",
    "\n",
    "    # close open Datasets\n",
    "    grid_out.close(), grid_in.close(), rg_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Spatial Mean and Standard Deviation of Annual Near Surface Temperature\n",
    "\n",
    "Metacode:\n",
    "1. Define pathway for spatial annual near surface temperature data.\n",
    "2. For each period/scenario, concatenate all model output along a new dimension. (Model output is in the form of two DataArray objects; spatial mean annual near surface temperature and standard deviation of annual near surface temperature/inter-annual variability.)\n",
    "3. Compute the spatial mean along this new dimension for both DataArray objects (ensemble spatial means of annual near surface temperature and inter-annual near surface temperature variability).\n",
    "4. Compute the spatial standard deviation along this new dimension for spatial mean DataArray object (measure of ensemble variance).\n",
    "5. Form Dataset object from ensemble means and standard deviation DataArray objects from all periods/scenarios.\n",
    "6. Save Dataset object to suitable directory.\n",
    "\n",
    "Above is to be applied to both absolute and anomaly tropical nights 1x1 degree resolution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_cur = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/current/*/*/*anomaly*_1x1*')\n",
    "anom_126 = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/ssp126/*/*/*anomaly*_1x1*')\n",
    "anom_245 = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/ssp245/*/*/*anomaly*_1x1*')\n",
    "anom_370 = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/ssp370/*/*/*anomaly*_1x1*')\n",
    "anom_585 = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/ssp585/*/*/*anomaly*_1x1*')\n",
    "abso_pre = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/pre_ind/*/*/*ann_tas*_1x1*')\n",
    "abso_cur = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/current/*/*/*ann_tas*_1x1*')\n",
    "abso_126 = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/ssp126/*/*/*ann_tas*_1x1*')\n",
    "abso_245 = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/ssp245/*/*/*ann_tas*_1x1*')\n",
    "abso_370 = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/ssp370/*/*/*ann_tas*_1x1*')\n",
    "abso_585 = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/ssp585/*/*/*ann_tas*_1x1*')\n",
    "\n",
    "f_pws_list = [anom_cur, anom_126, anom_245, anom_370, anom_585,\n",
    "              abso_pre, abso_cur, abso_126, abso_245, abso_370, abso_585]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: current_anom_all_models.nc\n",
      "File saved: ssp126_anom_all_models.nc\n",
      "File saved: ssp245_anom_all_models.nc\n",
      "File saved: ssp370_anom_all_models.nc\n",
      "File saved: ssp585_anom_all_models.nc\n",
      "File saved: pre_ind_abso_all_models.nc\n",
      "File saved: current_abso_all_models.nc\n",
      "File saved: ssp126_abso_all_models.nc\n",
      "File saved: ssp245_abso_all_models.nc\n",
      "File saved: ssp370_abso_all_models.nc\n",
      "File saved: ssp585_abso_all_models.nc\n",
      "File saved: ensemble_tas.nc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset object to store the spatial ensemble variables of surface temperature\n",
    "ens_ds = xr.Dataset()\n",
    "\n",
    "for pws_group in f_pws_list:\n",
    "    \n",
    "    # load in a single model output; remaining model outputs will be concatenated to this\n",
    "    x0 = xr.open_dataset(pws_group[0], use_cftime = True)\n",
    "    models = [x0.model] # note model used\n",
    "    \n",
    "    # ensure all Datasets use same coordinate set by losing excess height coordinate if present\n",
    "    if 'height' in x0.coords:\n",
    "        x0 = x0.drop('height')\n",
    "    \n",
    "    # concatenate remaining model ouputs along a new dimension\n",
    "    for pw in pws_group[1: ]:\n",
    "        x = xr.open_dataset(pw, use_cftime = True)\n",
    "        if 'height' in x.coords: # drop excess height coordinate\n",
    "            x = x.drop('height')\n",
    "        x0 = xr.concat([x0, x], dim = 'model')\n",
    "        \n",
    "        # note model concatenated\n",
    "        models += [x.model]\n",
    "        \n",
    "        # close Dataset object\n",
    "        x.close()\n",
    "        del(x)\n",
    "    \n",
    "    # update attribute information to concatenated Dataset\n",
    "    x0.attrs['model_index'] = models # add names of model concatenated in order of concatenation;\n",
    "                                     # index of model dimension will match names in list\n",
    "    if x0.experiment == 'historical' and x0.period == '1981-2010': # distinguish between historical periods\n",
    "        x0.attrs['experiment'] = 'current'\n",
    "    if x0.experiment == 'historical' and x0.period == '1851-1900':\n",
    "        x0.attrs['experiment'] = 'pre_ind'\n",
    "    if 'anomaly' in pw:\n",
    "        x0.attrs['data_type'] = 'anomaly'\n",
    "    if 'ann_tas' in pw:\n",
    "        x0.attrs['data_type'] = 'absolute'\n",
    "    del(x0.attrs['model'], x0.attrs['modelling_group'], x0.attrs['realisation'])\n",
    "    \n",
    "    # save Dataset object containing the concatenated model output for specfic group\n",
    "    save_pw = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/processed/' + \\\n",
    "              f'{x0.experiment}_{x0.data_type[0: 4]}_all_models.nc'\n",
    "    x0.to_netcdf(save_pw, 'w')\n",
    "    print(f'File saved: {save_pw.split(\"/\")[-1]}')\n",
    "    del(save_pw)\n",
    "    \n",
    "    # create a dictionary object storing attribute information for new objects\n",
    "    attrs_info = {'models': models, 'models_used': len(models), 'regird_method': x0.regrid_method,\n",
    "                  'data_type': x0.data_type, 'experiment': x0.experiment, 'units': x0.units,\n",
    "                  'period': x0.period, 'resolution': x0.resolution}\n",
    "\n",
    "    # compute the ensemble spatial annual near surface temperature variables and add attribute information\n",
    "    if 'anomaly' in x0.data_type:\n",
    "        ens_mean = x0.ann_tas_anom.mean(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "        ens_intra = x0.std_dev_ann_tas_anom.mean(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "        ens_std = x0.ann_tas_anom.std(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "        ens_qua = x0.ann_tas_anom.quantile([0.05, 0.25, 0.75, 0.95], dim = 'model',\n",
    "                                               interpolation = 'linear').assign_attrs(attrs_info)\n",
    "        ens_medi = x0.ann_tas_anom.median(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "    \n",
    "    elif 'absolute' in x0.data_type:\n",
    "        ens_mean = x0.mean_tas.mean(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "        ens_intra = x0.std_dev_tas.mean(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "        ens_std = x0.mean_tas.std(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "        ens_qua = x0.mean_tas.quantile([0.05, 0.25, 0.75, 0.95], dim = 'model',\n",
    "                                          interpolation = 'linear').assign_attrs(attrs_info)\n",
    "        ens_medi = x0.mean_tas.median(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('check data type attribute of Datasets being averaged.')\n",
    "        \n",
    "    # add DataArray objects to ensemble Dataset object\n",
    "    ens_ds = ens_ds.assign({f'{x0.experiment}_mean_ann_tas_{x0.data_type[0: 4]}': ens_mean})\n",
    "    ens_ds = ens_ds.assign({f'{x0.experiment}_intrvar_ann_tas_{x0.data_type[0: 4]}': ens_intra})\n",
    "    ens_ds = ens_ds.assign({f'{x0.experiment}_std_ann_tas_{x0.data_type[0: 4]}': ens_std})\n",
    "    ens_ds = ens_ds.assign({f'{x0.experiment}_quantiles_ann_tas_{x0.data_type[0: 4]}': ens_qua})\n",
    "    ens_ds = ens_ds.assign({f'{x0.experiment}_medi_ann_tas_{x0.data_type[0: 4]}': ens_medi})\n",
    "    \n",
    "# update attribute informaton to Dataset object\n",
    "ens_ds = ens_ds.assign_attrs(attrs_info)\n",
    "del(ens_ds.attrs['data_type'], ens_ds.attrs['period'], ens_ds.attrs['experiment'])\n",
    "ens_ds.attrs['description'] = '\"mean_ann_tas\" = spatial ensemble mean annual near surface temperature; ' +\\\n",
    "                              '\"intrvar_ann_tas\" = spatial ensemble mean model inter-annual variabiltiy in '+\\\n",
    "                              'annual near surface temperature; \"std_ann_tas\" = spatial ensemble standard '+\\\n",
    "                              'deviation in annual near surface temperature; \"quantiles_ann_tas\" = 5th, 25th, 75th '+\\\n",
    "                              'and 95th percentiles of ensemble annual near surface temperature; \"medi_ann_tas\" = '+\\\n",
    "                              'spatial ensemble median annual near surface temperatures.'\n",
    "    \n",
    "# save Dataset object\n",
    "save_pw = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/processed/ensemble_tas.nc'\n",
    "ens_ds.to_netcdf(save_pw, 'w')\n",
    "print(f'File saved: {save_pw.split(\"/\")[-1]}')\n",
    "\n",
    "# close DataArray/set objects\n",
    "x0.close(), ens_ds.close(), ens_mean.close(), ens_intra.close(), ens_std.close(),\n",
    "ens_qua.close(), ens_medi.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-model ensemble for the current period only is to be regridded to 5x5 latitude-longitude grid for use with an observational dataset (CRUTEM4) for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: curret_abso_all_models_5x5_res.nc\n",
      "File saved: ensemble_tas_5x5_res.nc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset object to store the spatial ensemble mean annual tropical nights variable\n",
    "ens_ds = xr.Dataset()\n",
    "\n",
    "# define pathway to current absolute 5x5 resolution model output\n",
    "abso_cur = glob.glob('/home/ucfagtj/DATA/Dissertation/Data/Temperature/Annual/current/*/*/*ann_tas*5x5*')\n",
    "    \n",
    "# load in a single model output; remaining model outputs will be concatenated to this\n",
    "x0 = xr.open_dataset(abso_cur[0], use_cftime = True)\n",
    "models = [x0.model] # note model used\n",
    "\n",
    "# ensure all Datasets use same coordinate set by losing excess height coordinate if present\n",
    "if 'height' in x0.coords:\n",
    "    x0 = x0.drop('height')\n",
    "    \n",
    "# concatenate remaining model ouputs along a new dimension\n",
    "for pw in abso_cur[1: ]:\n",
    "    x = xr.open_dataset(pw, use_cftime = True)\n",
    "    x0 = xr.concat([x0, x], dim = 'model')\n",
    "    if 'height' in x.coords: # drop excess height coordinate\n",
    "        x = x.drop('height')\n",
    "        \n",
    "    # note model concatenated\n",
    "    models += [x.model]\n",
    "        \n",
    "    # close Dataset object\n",
    "    x.close()\n",
    "    del(x)\n",
    "        \n",
    "# update attribute information to concatenated Dataset\n",
    "x0.attrs['model_index'] = models # add names of model concatenated in order of concatenation;\n",
    "                                 # index of model dimension will match names in list\n",
    "del(x0.attrs['model'], x0.attrs['modelling_group'], x0.attrs['realisation'])\n",
    "        \n",
    "# save Dataset object containing the concatenated model output for specfic group\n",
    "save_pw = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/processed/' + \\\n",
    "           'curret_abso_all_models_5x5_res.nc'\n",
    "x0.to_netcdf(save_pw, 'w')\n",
    "print(f'File saved: {save_pw.split(\"/\")[-1]}')\n",
    "del(save_pw)\n",
    "    \n",
    "# create a dictionary object storing attribute information for new objects\n",
    "attrs_info = {'models': models, 'models_used': len(models), 'regird_method': x0.regrid_method,\n",
    "              'data_type': 'absolute', 'experiment': x0.experiment,\n",
    "              'year_sampled': '30', 'period': 'current', 'resolution': x0.resolution}\n",
    "\n",
    "# compute the ensemble spatial annual tropical night variables and add attribute information\n",
    "ens_mean = x0.mean_tas.mean(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "ens_intra = x0.std_dev_tas.mean(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "ens_std = x0.mean_tas.std(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "ens_qua = x0.mean_tas.quantile([0.05, 0.25, 0.75, 0.95], dim = 'model',\n",
    "                                interpolation = 'linear').assign_attrs(attrs_info)\n",
    "ens_medi = x0.mean_tas.median(dim = 'model', skipna = True).assign_attrs(attrs_info)\n",
    "       \n",
    "# add DataArray objects to ensemble Dataset object\n",
    "ens_ds = ens_ds.assign({'current_mean_ann_tas_abso': ens_mean})\n",
    "ens_ds = ens_ds.assign({'current_intrvar_ann_tas_abso': ens_intra})\n",
    "ens_ds = ens_ds.assign({'current_std_ann_tas_abso': ens_std})\n",
    "ens_ds = ens_ds.assign({'current_quantiles_ann_tas_abso': ens_qua})\n",
    "ens_ds = ens_ds.assign({'current_medi_ann_tas_abso': ens_medi})\n",
    "    \n",
    "# update attribute informaton to Dataset object\n",
    "ens_ds = ens_ds.assign_attrs(attrs_info)\n",
    "del(ens_ds.attrs['data_type'], ens_ds.attrs['period'], ens_ds.attrs['experiment'],\n",
    "    ens_ds.attrs['year_sampled'])\n",
    "ens_ds.attrs['description'] = '\"mean_ann_tas\" = spatial ensemble mean annual near surface temperature; ' +\\\n",
    "                              '\"intrvar_ann_tas\" = spatial ensemble mean model inter-annual variabiltiy in '+\\\n",
    "                              'annual near surface temperature; \"std_ann_tas\" = spatial ensemble standard '+\\\n",
    "                              'deviation in annual near surface temperature; \"quantiles_ann_tas\" = 5th, 25th, 75th '+\\\n",
    "                              'and 95th percentiles of ensemble annual near surface temperature; \"medi_ann_tas\" = '+\\\n",
    "                              'spatial ensemble median annual near surface temperatures.'\n",
    "\n",
    "# save Dataset object\n",
    "save_pw = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/processed/ensemble_tas_5x5_res.nc'\n",
    "ens_ds.to_netcdf(save_pw, 'w')\n",
    "print(f'File saved: {save_pw.split(\"/\")[-1]}')\n",
    "\n",
    "# close DataArray/set objects\n",
    "x0.close(), ens_ds.close(), ens_mean.close(), ens_intra.close(), ens_std.close(),\n",
    "ens_qua.close(), ens_medi.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Regional Ensemble Means\n",
    "\n",
    "As regional and surface means will use raster cells of differing areas, a weighting must be applied such that cell values with a larger area contribute more towards the mean. For rectangular grid data, the cosine of the latitude is a good approximation to provide a weighting.\n",
    "\n",
    "Metacode:\n",
    "\n",
    "1. Create a weighted mean function. This function will take 3 inputs; a data array, a weights array, and dimension/s to compute the weighted mean over.\n",
    "2. Define regions to calculate weighted means for.\n",
    "3. Create a weights array using latitude cosine approximation. Use this weights array to assign zero weights to cells outside desired region.\n",
    "4. Compute weighted means for each region for each DataArray object.\n",
    "5. Populate a DataFrame object with weighted means.\n",
    "6. Save DataFrame object using serialisation for efficiency (\"pickling\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(arr, weights, dim):\n",
    "    '''\n",
    "    Summary:\n",
    "    --------\n",
    "    Computes a weighted mean of an array along a dimension/dimensions of a DataArray object.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr: xarray DataArray object\n",
    "         array containing values used to compute a weighted mean\n",
    "         \n",
    "    weights: xarray DataArray object\n",
    "             array containing weights to be applied to the values within arr object\n",
    "             \n",
    "    dim: str or sequence of str\n",
    "         dimension/s over which to compute the weighted mean\n",
    "         \n",
    "    Returns:\n",
    "    --------\n",
    "    weighted_mean: xarray DataArray oject\n",
    "                   array containing weighted mean with the dimension/s mean calculated over removed.\n",
    "                   \n",
    "    '''\n",
    "    \n",
    "    # sum up the weighted sum of the values within the region specified by dim\n",
    "    # recall matrix multiplication; cols of first equal rows of second; (10x2) * (2x6) = (10x6)\n",
    "    # so weights array does not need to be same shape as data array\n",
    "    weighted_sum = (arr * weights).sum(dim = dim, skipna = True)\n",
    "    \n",
    "    # sum up the values of the weights within the region specified by dim\n",
    "    # define an array where weights of cells with valid and invalid values are preserved and NA respectively\n",
    "    masked_weights = weights.where(arr.notnull()) \n",
    "    \n",
    "    # sum up the weights of the valid cells\n",
    "    sum_of_weights = masked_weights.sum(dim = dim, skipna = True)\n",
    "    \n",
    "    # as cannot divide by zero, set weights equal to zero as NA\n",
    "    # the values of these cells will be removed from the mean in the weighted sum part (multiplied by 0)\n",
    "    valid_weights = sum_of_weights != 0\n",
    "    sum_of_weights = sum_of_weights.where(valid_weights)\n",
    "    \n",
    "    # compute weighted mean along the specified dimension/s\n",
    "    weighted_mean = weighted_sum / sum_of_weights\n",
    "    \n",
    "    return weighted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_average(arr, regions, land_only = True):\n",
    "    '''\n",
    "    Summary:\n",
    "    --------\n",
    "    Computes weighted mean of an array for various regions, as well for the globe, the ocean, \n",
    "    and the land with and without Antartica.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr: xarray DataArray object\n",
    "         array encompassing the regions where the weighted means are computed\n",
    "    \n",
    "    regions: regionmask.Regions object\n",
    "             regions to compute the weighted means over\n",
    "        \n",
    "    land_only : bool\n",
    "                whether to mask out ocean points before calculating regional means\n",
    "                default is True\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    reg_ave : xarray DataArray object\n",
    "              New DataArray with weighted mean over the whole globe, the ocean, the land, the \n",
    "              land without Antarctica, and all regions.\n",
    "              Dimensions (n_regions + 4) x (additional dimensions no averaged over)\n",
    "    \n",
    "    '''\n",
    "    # check that regions specified are an instance of regionmask.Regions;\n",
    "    # essnetially checking all are an 'regionmask.Regions' object type\n",
    "    if not isinstance(regions, regionmask.Regions):\n",
    "        raise ValueError('specified regions must be a regionmask.Regions instance')\n",
    "        \n",
    "    # define names of regions to be used to index the various weighted means to be computed\n",
    "    abbrevs = ['global', 'ocean', 'land', 'land_wo_antarctica']\n",
    "    abbrevs = abbrevs + regions.abbrevs\n",
    "    \n",
    "    # define the IPCC numbers for each regions to index the various weighted means to be computed\n",
    "    numbers = np.array(regions.numbers)\n",
    "    \n",
    "    # compute the latitude weights using the cosine approximation\n",
    "    weight = np.cos(np.deg2rad(arr.lat))\n",
    "    \n",
    "    # define a land mask where land and sea cells are True and False respectively\n",
    "    landmask = regionmask.defined_regions.natural_earth.land_110.mask(arr)\n",
    "    landmask = landmask == 0\n",
    "    \n",
    "    # for land only, combine latitude weighting with landmask\n",
    "    # result being that only cells over land have non-zero weights following latitude cosine approximation\n",
    "    if land_only:\n",
    "        wgt = weight * landmask\n",
    "    \n",
    "    # otherwise, combine latitude weighting with same shape as input array\n",
    "    # result being that all cells, both land or ocean, will have a weight corresponding to its latitude\n",
    "    # this weight will be used for regions containing both ocean and land cells\n",
    "    else:\n",
    "        wgt = xr.full_like(landmask, 1) * weight\n",
    "    \n",
    "    # define a region mask; cells given number for a given region (Europe = 1, Aus = 2 etc.)\n",
    "    # cells that do not fall into a region, denoted as NaN\n",
    "    mask = regions.mask(arr)\n",
    "    \n",
    "    # define a list to accumulate averages/weighted means\n",
    "    ave = list()\n",
    "    \n",
    "    # compute global mean\n",
    "    # weighting is simple cosine latitude weighting; no mask as averging entire surface/all cells\n",
    "    a = weighted_mean(arr, dim = ('lat', 'lon'), weights = weight)\n",
    "    ave.append(a)\n",
    "    \n",
    "    # compute global ocean mean\n",
    "    # weighting is a cosine latitude weighting of only ocean cells; land cells all weighted as 0\n",
    "    weights = (weight * (1.0 - landmask))\n",
    "    a = weighted_mean(arr, dim = ('lat', 'lon'), weights = weights)\n",
    "    ave.append(a)\n",
    "    \n",
    "    # compute global land mean\n",
    "    # weighting is a cosine latitude weighting of only land cells; ocean cells weighted as 0\n",
    "    weights = (weight * landmask)\n",
    "    a = weighted_mean(arr, dim = ('lat', 'lon'), weights = weights)\n",
    "    ave.append(a)\n",
    "    \n",
    "    # compute global land mean without Antarctica\n",
    "    # weighting is a cosine latitude weighting of only land cells; ocean cells weighted as 0\n",
    "    arr_selected = arr.sel(lat = slice(-60, None)) # remove Antarctica by removing low latitudes\n",
    "    weights = (weight * landmask)\n",
    "    a = weighted_mean(arr_selected, dim = ('lat', 'lon'), weights = weights)\n",
    "    ave.append(a)\n",
    "    \n",
    "    #### Regional Weighted Means ####\n",
    "    # Computing the specified regional averages is quicker using Groupby objects\n",
    "    # Groupby objects use multi indexing of coordinates to state the raster cells of a given group\n",
    "    # Multi indexing essentially concatenates the coordinates\n",
    "    # (i.e. a cell with lat = -5 and lon = 25 denoted as lat_lon = -5, 25)\n",
    "    # Using these \"stacked\" coordinates reduces the dimensions/shape of an array\n",
    "    # (i.e. stacking lat and lon will change the 2D representation of these to 1D)\n",
    "    \n",
    "    # compute the region weighted means\n",
    "    g = arr.groupby(mask) # group array into the different regions\n",
    "    \n",
    "    # create a new dimension of 'stacked coordinates'; moves from raster/grid format to 1D object\n",
    "    # (unstacked dimensions -> lat = 10, lon = 12, value = 50 stacked dimension -> 10_12, value = 50)\n",
    "    wgt_stacked = wgt.stack(stacked_lat_lon = ('lat', 'lon'))\n",
    "    \n",
    "    # apply stacked lat_lon weights to stacked Groupby object\n",
    "    a = g.apply(weighted_mean, dim = ('stacked_lat_lon'), weights = wgt_stacked)\n",
    "\n",
    "    ave.append(a.drop('region')) # drop the region information as want to use as dimension to merge averages\n",
    "    \n",
    "    # merge the list of weighted means DataArray objects into a single DataArray object\n",
    "    arr = xr.concat(ave, dim = 'region')\n",
    "    \n",
    "    # shift region coordinates such that the numbers correspond to the regions\n",
    "    # accounting for the 4 non-regional weighted means also computed\n",
    "    numbers = np.arange(numbers.min() - 4, numbers.max() + 1)\n",
    "    \n",
    "    # add the abbreviations of the regions and update the numbers\n",
    "    arr = arr.assign_coords(**{'abbrev': ('region', abbrevs), 'number': ('region', numbers)})\n",
    "    \n",
    "    # create a multi index\n",
    "    arr = arr.set_index(region = ('abbrev', 'number'))\n",
    "    \n",
    "    return arr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: ensemble_tas_variables_AR6.pickle\n"
     ]
    }
   ],
   "source": [
    "# load in spatial ensemble annual near surface temperature variables; has both absolute and anomaly\n",
    "f_pw = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/processed/ensemble_tas.nc'\n",
    "data = xr.open_dataset(f_pw)\n",
    "\n",
    "# define AR6 land regions to calculate weighted means for; must be regionmask objects\n",
    "regions = regionmask.defined_regions.ar6.land\n",
    "\n",
    "# create DataFrame object to store weighted mean\n",
    "wm_df = pd.DataFrame()\n",
    "\n",
    "# define column headers\n",
    "col_names = ['global', 'ocean', 'land', 'land_wo_antarctica']\n",
    "col_names = col_names + regions.abbrevs\n",
    "\n",
    "# add column headers to DataDrame object\n",
    "wm_df = wm_df.reindex(columns = col_names)\n",
    "\n",
    "# create a list of the various DataArray objects\n",
    "data_vars = data.data_vars.values() # .data_vars gives a dictionary object; .values() lists the DataArrays\n",
    "\n",
    "# loop over each DataArray object\n",
    "for data_arr in data_vars:\n",
    "    \n",
    "    # compute the regional weighted means; use only land cells for mean with regions with both ocean and land\n",
    "    w_means = region_average(data_arr, regions, land_only = True)\n",
    "    \n",
    "    # must loop over each percentile with quantile DataArray\n",
    "    if 'quantiles' in data_arr.name:\n",
    "        for i in range(0, 4):\n",
    "            \n",
    "            # add a row to DataFrame object for each percentile\n",
    "            row_name = f'{data_arr.name}_pct_{str(data_arr[\"quantile\"][i].values)[2:]}'\n",
    "            wm_df = wm_df.append(pd.Series(name = row_name, dtype = 'float64'))\n",
    "            \n",
    "            # populate the DataFrame object for each percentile\n",
    "            for col_name in col_names:\n",
    "                wm_df[col_name][row_name] = w_means[i, :].sel(abbrev = col_name)\n",
    "    \n",
    "    elif 'quantiles' not in data_arr.name:\n",
    "        \n",
    "        # add a row to DataFrame object\n",
    "        row_name = data_arr.name\n",
    "        wm_df = wm_df.append(pd.Series(name = row_name, dtype = 'float64'))\n",
    "      \n",
    "        # populate the DataFrame object\n",
    "        for col_name in col_names:\n",
    "            wm_df[col_name][row_name] = w_means.sel(abbrev = col_name)\n",
    "            \n",
    "# save DataFrame object using pickle; deconstructs and reconstucts data to save space\n",
    "save_pw = '/home/ucfagtj/DATA/Dissertation/Data/Temperature/processed/ensemble_tas_variables_AR6.pickle'\n",
    "with open(save_pw, 'wb') as f:\n",
    "        pickle.dump(wm_df, f) \n",
    "print(f'File saved: {save_pw.split(\"/\")[-1]}')\n",
    "\n",
    "# close open Dataset and/or DataArray objects    \n",
    "data.close()\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table View of Weighted Regional Ensemble Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_ind_mean_ann_tas_abso     8.480703\n",
      "current_mean_ann_tas_abso     9.172610\n",
      "ssp126_mean_ann_tas_abso     11.011854\n",
      "ssp370_mean_ann_tas_abso     13.455468\n",
      "Name: land, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(ar6_df['land'][[40, 48, 56, 72]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19    1.39\n",
      "47    1.55\n",
      "46    1.69\n",
      "Name: ssp126_pct, dtype: float64 32    9.51\n",
      "6     8.88\n",
      "5     7.99\n",
      "Name: ssp585_pct, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>pre_ind</th>\n",
       "      <th>current</th>\n",
       "      <th>ssp126</th>\n",
       "      <th>ssp245</th>\n",
       "      <th>ssp370</th>\n",
       "      <th>ssp585</th>\n",
       "      <th>current_pct</th>\n",
       "      <th>ssp126_pct</th>\n",
       "      <th>ssp245_pct</th>\n",
       "      <th>ssp370_pct</th>\n",
       "      <th>ssp585_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>global</td>\n",
       "      <td>13.74</td>\n",
       "      <td>14.30</td>\n",
       "      <td>15.66</td>\n",
       "      <td>16.52</td>\n",
       "      <td>17.47</td>\n",
       "      <td>18.16</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.78</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ocean</td>\n",
       "      <td>15.87</td>\n",
       "      <td>16.38</td>\n",
       "      <td>17.54</td>\n",
       "      <td>18.29</td>\n",
       "      <td>19.11</td>\n",
       "      <td>19.68</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.42</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>land</td>\n",
       "      <td>8.48</td>\n",
       "      <td>9.17</td>\n",
       "      <td>11.01</td>\n",
       "      <td>12.16</td>\n",
       "      <td>13.46</td>\n",
       "      <td>14.42</td>\n",
       "      <td>0.69</td>\n",
       "      <td>2.53</td>\n",
       "      <td>3.68</td>\n",
       "      <td>4.97</td>\n",
       "      <td>5.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>land_wo_antarctica</td>\n",
       "      <td>12.51</td>\n",
       "      <td>13.19</td>\n",
       "      <td>15.07</td>\n",
       "      <td>16.24</td>\n",
       "      <td>17.55</td>\n",
       "      <td>18.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>2.57</td>\n",
       "      <td>3.73</td>\n",
       "      <td>5.04</td>\n",
       "      <td>6.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GIC</td>\n",
       "      <td>-19.78</td>\n",
       "      <td>-18.75</td>\n",
       "      <td>-16.55</td>\n",
       "      <td>-15.17</td>\n",
       "      <td>-13.76</td>\n",
       "      <td>-12.80</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.61</td>\n",
       "      <td>6.02</td>\n",
       "      <td>6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NWN</td>\n",
       "      <td>-5.03</td>\n",
       "      <td>-4.13</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.90</td>\n",
       "      <td>3.60</td>\n",
       "      <td>5.01</td>\n",
       "      <td>6.65</td>\n",
       "      <td>7.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NEN</td>\n",
       "      <td>-7.18</td>\n",
       "      <td>-6.26</td>\n",
       "      <td>-3.23</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.92</td>\n",
       "      <td>3.95</td>\n",
       "      <td>5.56</td>\n",
       "      <td>7.42</td>\n",
       "      <td>8.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WNA</td>\n",
       "      <td>7.58</td>\n",
       "      <td>8.05</td>\n",
       "      <td>10.10</td>\n",
       "      <td>11.28</td>\n",
       "      <td>12.51</td>\n",
       "      <td>13.51</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.92</td>\n",
       "      <td>5.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CNA</td>\n",
       "      <td>11.66</td>\n",
       "      <td>11.96</td>\n",
       "      <td>14.16</td>\n",
       "      <td>15.42</td>\n",
       "      <td>16.69</td>\n",
       "      <td>17.73</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.49</td>\n",
       "      <td>3.76</td>\n",
       "      <td>5.03</td>\n",
       "      <td>6.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ENA</td>\n",
       "      <td>9.41</td>\n",
       "      <td>9.83</td>\n",
       "      <td>12.02</td>\n",
       "      <td>13.22</td>\n",
       "      <td>14.53</td>\n",
       "      <td>15.51</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.82</td>\n",
       "      <td>5.12</td>\n",
       "      <td>6.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NCA</td>\n",
       "      <td>18.32</td>\n",
       "      <td>18.87</td>\n",
       "      <td>20.46</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.73</td>\n",
       "      <td>23.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.41</td>\n",
       "      <td>5.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SCA</td>\n",
       "      <td>23.69</td>\n",
       "      <td>24.34</td>\n",
       "      <td>25.67</td>\n",
       "      <td>26.60</td>\n",
       "      <td>27.75</td>\n",
       "      <td>28.48</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.91</td>\n",
       "      <td>4.06</td>\n",
       "      <td>4.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CAR</td>\n",
       "      <td>25.56</td>\n",
       "      <td>26.08</td>\n",
       "      <td>27.32</td>\n",
       "      <td>28.13</td>\n",
       "      <td>28.99</td>\n",
       "      <td>29.63</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.57</td>\n",
       "      <td>3.44</td>\n",
       "      <td>4.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NWS</td>\n",
       "      <td>21.27</td>\n",
       "      <td>21.97</td>\n",
       "      <td>23.35</td>\n",
       "      <td>24.36</td>\n",
       "      <td>25.62</td>\n",
       "      <td>26.38</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3.09</td>\n",
       "      <td>4.35</td>\n",
       "      <td>5.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NSA</td>\n",
       "      <td>25.39</td>\n",
       "      <td>26.17</td>\n",
       "      <td>27.66</td>\n",
       "      <td>28.75</td>\n",
       "      <td>30.10</td>\n",
       "      <td>31.07</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.27</td>\n",
       "      <td>3.36</td>\n",
       "      <td>4.71</td>\n",
       "      <td>5.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NES</td>\n",
       "      <td>23.86</td>\n",
       "      <td>24.58</td>\n",
       "      <td>25.99</td>\n",
       "      <td>26.98</td>\n",
       "      <td>28.14</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.12</td>\n",
       "      <td>3.12</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SAM</td>\n",
       "      <td>22.60</td>\n",
       "      <td>23.42</td>\n",
       "      <td>25.08</td>\n",
       "      <td>26.29</td>\n",
       "      <td>27.69</td>\n",
       "      <td>28.72</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.48</td>\n",
       "      <td>3.69</td>\n",
       "      <td>5.09</td>\n",
       "      <td>6.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SWS</td>\n",
       "      <td>9.69</td>\n",
       "      <td>10.43</td>\n",
       "      <td>11.69</td>\n",
       "      <td>12.73</td>\n",
       "      <td>13.84</td>\n",
       "      <td>14.59</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.04</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SES</td>\n",
       "      <td>18.45</td>\n",
       "      <td>19.13</td>\n",
       "      <td>20.33</td>\n",
       "      <td>21.30</td>\n",
       "      <td>22.34</td>\n",
       "      <td>23.15</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.89</td>\n",
       "      <td>4.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SSA</td>\n",
       "      <td>7.29</td>\n",
       "      <td>7.81</td>\n",
       "      <td>8.68</td>\n",
       "      <td>9.41</td>\n",
       "      <td>10.18</td>\n",
       "      <td>10.72</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NEU</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.94</td>\n",
       "      <td>6.01</td>\n",
       "      <td>7.34</td>\n",
       "      <td>8.11</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.83</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>WCE</td>\n",
       "      <td>7.64</td>\n",
       "      <td>8.17</td>\n",
       "      <td>10.23</td>\n",
       "      <td>11.26</td>\n",
       "      <td>12.59</td>\n",
       "      <td>13.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.59</td>\n",
       "      <td>3.62</td>\n",
       "      <td>4.95</td>\n",
       "      <td>5.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>EEU</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.49</td>\n",
       "      <td>7.01</td>\n",
       "      <td>8.21</td>\n",
       "      <td>9.75</td>\n",
       "      <td>10.87</td>\n",
       "      <td>0.62</td>\n",
       "      <td>3.14</td>\n",
       "      <td>4.33</td>\n",
       "      <td>5.88</td>\n",
       "      <td>6.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MED</td>\n",
       "      <td>15.48</td>\n",
       "      <td>16.03</td>\n",
       "      <td>17.86</td>\n",
       "      <td>18.93</td>\n",
       "      <td>20.17</td>\n",
       "      <td>21.11</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.39</td>\n",
       "      <td>3.45</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SAH</td>\n",
       "      <td>24.12</td>\n",
       "      <td>24.95</td>\n",
       "      <td>26.66</td>\n",
       "      <td>27.93</td>\n",
       "      <td>29.30</td>\n",
       "      <td>30.21</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.81</td>\n",
       "      <td>5.18</td>\n",
       "      <td>6.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>WAF</td>\n",
       "      <td>26.52</td>\n",
       "      <td>27.20</td>\n",
       "      <td>28.59</td>\n",
       "      <td>29.63</td>\n",
       "      <td>30.65</td>\n",
       "      <td>31.44</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3.12</td>\n",
       "      <td>4.13</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CAF</td>\n",
       "      <td>24.68</td>\n",
       "      <td>25.32</td>\n",
       "      <td>26.67</td>\n",
       "      <td>27.70</td>\n",
       "      <td>28.82</td>\n",
       "      <td>29.41</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.02</td>\n",
       "      <td>4.14</td>\n",
       "      <td>4.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NEAF</td>\n",
       "      <td>25.09</td>\n",
       "      <td>25.80</td>\n",
       "      <td>27.18</td>\n",
       "      <td>28.16</td>\n",
       "      <td>29.16</td>\n",
       "      <td>29.86</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.09</td>\n",
       "      <td>3.07</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SEAF</td>\n",
       "      <td>22.18</td>\n",
       "      <td>22.83</td>\n",
       "      <td>24.10</td>\n",
       "      <td>25.11</td>\n",
       "      <td>26.12</td>\n",
       "      <td>26.78</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.93</td>\n",
       "      <td>3.94</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>WSAF</td>\n",
       "      <td>20.32</td>\n",
       "      <td>21.11</td>\n",
       "      <td>22.64</td>\n",
       "      <td>23.83</td>\n",
       "      <td>25.15</td>\n",
       "      <td>25.93</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.32</td>\n",
       "      <td>3.52</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ESAF</td>\n",
       "      <td>20.49</td>\n",
       "      <td>21.20</td>\n",
       "      <td>22.64</td>\n",
       "      <td>23.78</td>\n",
       "      <td>24.95</td>\n",
       "      <td>25.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.14</td>\n",
       "      <td>3.29</td>\n",
       "      <td>4.46</td>\n",
       "      <td>5.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MDG</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.79</td>\n",
       "      <td>23.94</td>\n",
       "      <td>24.83</td>\n",
       "      <td>25.80</td>\n",
       "      <td>26.44</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.66</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RAR</td>\n",
       "      <td>-11.41</td>\n",
       "      <td>-10.34</td>\n",
       "      <td>-7.08</td>\n",
       "      <td>-5.39</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>1.08</td>\n",
       "      <td>4.34</td>\n",
       "      <td>6.03</td>\n",
       "      <td>8.01</td>\n",
       "      <td>9.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>WSB</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.93</td>\n",
       "      <td>4.40</td>\n",
       "      <td>5.75</td>\n",
       "      <td>7.35</td>\n",
       "      <td>8.63</td>\n",
       "      <td>0.69</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.50</td>\n",
       "      <td>6.10</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ESB</td>\n",
       "      <td>-4.18</td>\n",
       "      <td>-3.49</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.69</td>\n",
       "      <td>3.03</td>\n",
       "      <td>4.34</td>\n",
       "      <td>5.93</td>\n",
       "      <td>7.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RFE</td>\n",
       "      <td>-6.46</td>\n",
       "      <td>-5.71</td>\n",
       "      <td>-3.13</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.32</td>\n",
       "      <td>4.71</td>\n",
       "      <td>6.32</td>\n",
       "      <td>7.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>WCA</td>\n",
       "      <td>13.62</td>\n",
       "      <td>14.26</td>\n",
       "      <td>16.31</td>\n",
       "      <td>17.51</td>\n",
       "      <td>18.83</td>\n",
       "      <td>19.91</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.89</td>\n",
       "      <td>5.22</td>\n",
       "      <td>6.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ECA</td>\n",
       "      <td>4.78</td>\n",
       "      <td>5.38</td>\n",
       "      <td>7.43</td>\n",
       "      <td>8.69</td>\n",
       "      <td>10.06</td>\n",
       "      <td>11.21</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.91</td>\n",
       "      <td>5.27</td>\n",
       "      <td>6.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>TIB</td>\n",
       "      <td>-2.80</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.34</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.59</td>\n",
       "      <td>3.77</td>\n",
       "      <td>5.15</td>\n",
       "      <td>6.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>EAS</td>\n",
       "      <td>11.52</td>\n",
       "      <td>11.80</td>\n",
       "      <td>13.73</td>\n",
       "      <td>14.72</td>\n",
       "      <td>15.70</td>\n",
       "      <td>16.79</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.21</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4.18</td>\n",
       "      <td>5.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ARP</td>\n",
       "      <td>24.36</td>\n",
       "      <td>25.17</td>\n",
       "      <td>26.97</td>\n",
       "      <td>28.22</td>\n",
       "      <td>29.57</td>\n",
       "      <td>30.54</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.61</td>\n",
       "      <td>3.85</td>\n",
       "      <td>5.20</td>\n",
       "      <td>6.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SAS</td>\n",
       "      <td>23.78</td>\n",
       "      <td>24.29</td>\n",
       "      <td>25.76</td>\n",
       "      <td>26.65</td>\n",
       "      <td>27.63</td>\n",
       "      <td>28.63</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SEA</td>\n",
       "      <td>24.89</td>\n",
       "      <td>25.44</td>\n",
       "      <td>26.62</td>\n",
       "      <td>27.44</td>\n",
       "      <td>28.31</td>\n",
       "      <td>28.97</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.42</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NAU</td>\n",
       "      <td>26.03</td>\n",
       "      <td>26.76</td>\n",
       "      <td>28.13</td>\n",
       "      <td>29.10</td>\n",
       "      <td>30.16</td>\n",
       "      <td>30.92</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.08</td>\n",
       "      <td>4.13</td>\n",
       "      <td>4.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>CAU</td>\n",
       "      <td>22.57</td>\n",
       "      <td>23.38</td>\n",
       "      <td>24.80</td>\n",
       "      <td>25.92</td>\n",
       "      <td>27.25</td>\n",
       "      <td>28.08</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2.23</td>\n",
       "      <td>3.36</td>\n",
       "      <td>4.68</td>\n",
       "      <td>5.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>EAU</td>\n",
       "      <td>19.17</td>\n",
       "      <td>19.88</td>\n",
       "      <td>21.19</td>\n",
       "      <td>22.14</td>\n",
       "      <td>23.30</td>\n",
       "      <td>24.10</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.97</td>\n",
       "      <td>4.13</td>\n",
       "      <td>4.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SAU</td>\n",
       "      <td>16.68</td>\n",
       "      <td>17.24</td>\n",
       "      <td>18.36</td>\n",
       "      <td>19.24</td>\n",
       "      <td>20.29</td>\n",
       "      <td>20.94</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.57</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NZ</td>\n",
       "      <td>11.14</td>\n",
       "      <td>11.75</td>\n",
       "      <td>12.69</td>\n",
       "      <td>13.54</td>\n",
       "      <td>14.37</td>\n",
       "      <td>14.91</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                region  pre_ind  current  ssp126  ssp245  ssp370  ssp585  \\\n",
       "0               global    13.74    14.30   15.66   16.52   17.47   18.16   \n",
       "1                ocean    15.87    16.38   17.54   18.29   19.11   19.68   \n",
       "2                 land     8.48     9.17   11.01   12.16   13.46   14.42   \n",
       "3   land_wo_antarctica    12.51    13.19   15.07   16.24   17.55   18.52   \n",
       "4                  GIC   -19.78   -18.75  -16.55  -15.17  -13.76  -12.80   \n",
       "5                  NWN    -5.03    -4.13   -1.43   -0.01    1.63    2.96   \n",
       "6                  NEN    -7.18    -6.26   -3.23   -1.62    0.24    1.70   \n",
       "7                  WNA     7.58     8.05   10.10   11.28   12.51   13.51   \n",
       "8                  CNA    11.66    11.96   14.16   15.42   16.69   17.73   \n",
       "9                  ENA     9.41     9.83   12.02   13.22   14.53   15.51   \n",
       "10                 NCA    18.32    18.87   20.46   21.56   22.73   23.54   \n",
       "11                 SCA    23.69    24.34   25.67   26.60   27.75   28.48   \n",
       "12                 CAR    25.56    26.08   27.32   28.13   28.99   29.63   \n",
       "13                 NWS    21.27    21.97   23.35   24.36   25.62   26.38   \n",
       "14                 NSA    25.39    26.17   27.66   28.75   30.10   31.07   \n",
       "15                 NES    23.86    24.58   25.99   26.98   28.14   28.95   \n",
       "16                 SAM    22.60    23.42   25.08   26.29   27.69   28.72   \n",
       "17                 SWS     9.69    10.43   11.69   12.73   13.84   14.59   \n",
       "18                 SES    18.45    19.13   20.33   21.30   22.34   23.15   \n",
       "19                 SSA     7.29     7.81    8.68    9.41   10.18   10.72   \n",
       "20                 NEU     2.18     2.93    4.94    6.01    7.34    8.11   \n",
       "21                 WCE     7.64     8.17   10.23   11.26   12.59   13.50   \n",
       "22                 EEU     3.87     4.49    7.01    8.21    9.75   10.87   \n",
       "23                 MED    15.48    16.03   17.86   18.93   20.17   21.11   \n",
       "24                 SAH    24.12    24.95   26.66   27.93   29.30   30.21   \n",
       "25                 WAF    26.52    27.20   28.59   29.63   30.65   31.44   \n",
       "26                 CAF    24.68    25.32   26.67   27.70   28.82   29.41   \n",
       "27                NEAF    25.09    25.80   27.18   28.16   29.16   29.86   \n",
       "28                SEAF    22.18    22.83   24.10   25.11   26.12   26.78   \n",
       "29                WSAF    20.32    21.11   22.64   23.83   25.15   25.93   \n",
       "30                ESAF    20.49    21.20   22.64   23.78   24.95   25.71   \n",
       "31                 MDG    22.15    22.79   23.94   24.83   25.80   26.44   \n",
       "32                 RAR   -11.41   -10.34   -7.08   -5.39   -3.40   -1.90   \n",
       "33                 WSB     1.25     1.93    4.40    5.75    7.35    8.63   \n",
       "34                 ESB    -4.18    -3.49   -1.15    0.16    1.76    2.97   \n",
       "35                 RFE    -6.46    -5.71   -3.13   -1.75   -0.14    1.09   \n",
       "36                 WCA    13.62    14.26   16.31   17.51   18.83   19.91   \n",
       "37                 ECA     4.78     5.38    7.43    8.69   10.06   11.21   \n",
       "38                 TIB    -2.80    -2.12   -0.22    0.97    2.34    3.36   \n",
       "39                 EAS    11.52    11.80   13.73   14.72   15.70   16.79   \n",
       "40                 ARP    24.36    25.17   26.97   28.22   29.57   30.54   \n",
       "41                 SAS    23.78    24.29   25.76   26.65   27.63   28.63   \n",
       "42                 SEA    24.89    25.44   26.62   27.44   28.31   28.97   \n",
       "43                 NAU    26.03    26.76   28.13   29.10   30.16   30.92   \n",
       "44                 CAU    22.57    23.38   24.80   25.92   27.25   28.08   \n",
       "45                 EAU    19.17    19.88   21.19   22.14   23.30   24.10   \n",
       "46                 SAU    16.68    17.24   18.36   19.24   20.29   20.94   \n",
       "47                  NZ    11.14    11.75   12.69   13.54   14.37   14.91   \n",
       "\n",
       "    current_pct  ssp126_pct  ssp245_pct  ssp370_pct  ssp585_pct  \n",
       "0          0.56        1.92        2.78        3.74        4.42  \n",
       "1          0.51        1.67        2.42        3.24        3.81  \n",
       "2          0.69        2.53        3.68        4.97        5.93  \n",
       "3          0.69        2.57        3.73        5.04        6.02  \n",
       "4          1.03        3.22        4.61        6.02        6.98  \n",
       "5          0.90        3.60        5.01        6.65        7.99  \n",
       "6          0.92        3.95        5.56        7.42        8.88  \n",
       "7          0.46        2.52        3.70        4.92        5.92  \n",
       "8          0.30        2.49        3.76        5.03        6.07  \n",
       "9          0.42        2.62        3.82        5.12        6.10  \n",
       "10         0.55        2.15        3.25        4.41        5.23  \n",
       "11         0.65        1.98        2.91        4.06        4.79  \n",
       "12         0.52        1.76        2.57        3.44        4.07  \n",
       "13         0.70        2.08        3.09        4.35        5.11  \n",
       "14         0.78        2.27        3.36        4.71        5.68  \n",
       "15         0.72        2.12        3.12        4.27        5.08  \n",
       "16         0.81        2.48        3.69        5.09        6.12  \n",
       "17         0.74        2.00        3.04        4.15        4.90  \n",
       "18         0.68        1.88        2.86        3.89        4.71  \n",
       "19         0.52        1.39        2.13        2.89        3.44  \n",
       "20         0.75        2.76        3.83        5.15        5.93  \n",
       "21         0.53        2.59        3.62        4.95        5.86  \n",
       "22         0.62        3.14        4.33        5.88        6.99  \n",
       "23         0.55        2.39        3.45        4.70        5.63  \n",
       "24         0.84        2.55        3.81        5.18        6.09  \n",
       "25         0.68        2.08        3.12        4.13        4.92  \n",
       "26         0.64        1.99        3.02        4.14        4.72  \n",
       "27         0.71        2.09        3.07        4.07        4.77  \n",
       "28         0.66        1.93        2.93        3.94        4.61  \n",
       "29         0.79        2.32        3.52        4.83        5.61  \n",
       "30         0.71        2.14        3.29        4.46        5.22  \n",
       "31         0.64        1.80        2.69        3.66        4.30  \n",
       "32         1.08        4.34        6.03        8.01        9.51  \n",
       "33         0.69        3.16        4.50        6.10        7.38  \n",
       "34         0.69        3.03        4.34        5.93        7.15  \n",
       "35         0.75        3.32        4.71        6.32        7.54  \n",
       "36         0.64        2.69        3.89        5.22        6.29  \n",
       "37         0.60        2.64        3.91        5.27        6.42  \n",
       "38         0.68        2.59        3.77        5.15        6.16  \n",
       "39         0.28        2.21        3.20        4.18        5.27  \n",
       "40         0.81        2.61        3.85        5.20        6.18  \n",
       "41         0.51        1.98        2.87        3.85        4.85  \n",
       "42         0.55        1.73        2.55        3.42        4.08  \n",
       "43         0.74        2.10        3.08        4.13        4.90  \n",
       "44         0.82        2.23        3.36        4.68        5.52  \n",
       "45         0.71        2.03        2.97        4.13        4.93  \n",
       "46         0.57        1.69        2.57        3.61        4.27  \n",
       "47         0.61        1.55        2.40        3.23        3.77  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data weighted regional means data\n",
    "wm_pw = f'/home/ucfagtj/DATA/Dissertation/Data/Temperature/processed/ensemble_tas_variables_AR6.pickle'\n",
    "unpickle = open(wm_pw, 'rb')\n",
    "wm_df = pickle.load(unpickle)\n",
    "\n",
    "# create a DataFrame object with Regions and periods column\n",
    "col_names = ['region', 'pre_ind', 'current', 'ssp126', 'ssp245', 'ssp370', 'ssp585', \n",
    "             'current_pct', 'ssp126_pct', 'ssp245_pct', 'ssp370_pct', 'ssp585_pct']\n",
    "df = pd.DataFrame()\n",
    "df = df.reindex(columns = col_names)\n",
    "\n",
    "# define region names; excluding the two Antartica land regions\n",
    "region_names = ['global', 'ocean', 'land', 'land_wo_antarctica'] + regionmask.defined_regions.ar6.land.abbrevs[: -2] \n",
    "\n",
    "# extract absolute and percentage increase for each region\n",
    "for i, region in enumerate(region_names):\n",
    "    \n",
    "    # restrict weighted means DataFrame object to given region\n",
    "    reg_data = wm_df[f'{region}']\n",
    "    \n",
    "    # extract absolute values\n",
    "    pre_ind = reg_data['pre_ind_mean_ann_tas_abso']\n",
    "    current = reg_data['current_mean_ann_tas_abso']\n",
    "    ssp126 = reg_data['ssp126_mean_ann_tas_abso']\n",
    "    ssp245 = reg_data['ssp245_mean_ann_tas_abso']\n",
    "    ssp370 = reg_data['ssp370_mean_ann_tas_abso']\n",
    "    ssp585 = reg_data['ssp585_mean_ann_tas_abso']\n",
    "    \n",
    "    # extract pre-industrial anomaly values; no percentage changes as meaningless due to different T scales\n",
    "    current_pct = reg_data['current_mean_ann_tas_anom']\n",
    "    ssp126_pct = reg_data['ssp126_mean_ann_tas_anom']\n",
    "    ssp245_pct = reg_data['ssp245_mean_ann_tas_anom']\n",
    "    ssp370_pct = reg_data['ssp370_mean_ann_tas_anom']\n",
    "    ssp585_pct = reg_data['ssp585_mean_ann_tas_anom']\n",
    "\n",
    "    # from Dictionary object holding a given region's data\n",
    "    data = {'region': region, 'pre_ind': round(pre_ind, 2), 'current': round(current, 2),\n",
    "            'current_pct': round(current_pct, 2), 'ssp126': round(ssp126, 2), \n",
    "            'ssp126_pct': round(ssp126_pct, 2), 'ssp245': round(ssp245, 2),\n",
    "            'ssp245_pct': round(ssp245_pct, 2), 'ssp370': round(ssp370, 2),\n",
    "            'ssp370_pct': round(ssp370_pct, 2), 'ssp585': round(ssp585, 2),\n",
    "            'ssp585_pct': round(ssp585_pct, 2)}\n",
    "    \n",
    "    # add data as new entry to DataFrame object\n",
    "    df = df.append(data, ignore_index = True, sort = False)\n",
    "\n",
    "print(df['ssp126_pct'][4:].nsmallest(3), df['ssp585_pct'][4:].nlargest(3))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Region: Russian-Arctic (RAR / 28)\n",
       "center: [109.22387611  71.58559615]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate percentage increases from the pre-industrial\n",
    "#current_pct = (reg_data['current_mean_ann_tas_anom'] / abs(pre_ind)) * 100\n",
    "#ssp126_pct = (reg_data['ssp126_mean_ann_tas_anom'] / abs(pre_ind)) * 100\n",
    "#ssp245_pct = (reg_data['ssp245_mean_ann_tas_anom'] / abs(pre_ind)) * 100\n",
    "#ssp370_pct = (reg_data['ssp370_mean_ann_tas_anom'] / abs(pre_ind)) * 100\n",
    "#ssp585_pct = (reg_data['ssp585_mean_ann_tas_anom'] / abs(pre_ind)) * 100\n",
    "    \n",
    "regions = regionmask.defined_regions.ar6.land\n",
    "regions['RAR']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
